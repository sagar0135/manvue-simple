{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MANVUE Fashion Product Search with CLIP + FAISS + MongoDB\n",
        "## AI-Powered Visual Search for Men's Fashion E-commerce\n",
        "\n",
        "This notebook implements a complete visual search system using:\n",
        "- **CLIP** (Contrastive Language-Image Pre-training) for image embeddings\n",
        "- **FAISS** (Facebook AI Similarity Search) for fast similarity search\n",
        "- **MongoDB GridFS** for image storage and retrieval\n",
        "- **Fashion Product Dataset** for training and testing\n",
        "\n",
        "**Features:**\n",
        "- Upload user images and find similar products\n",
        "- Store product embeddings in FAISS index\n",
        "- Save images and metadata in MongoDB\n",
        "- Train traditional ML algorithms on CLIP embeddings\n",
        "- Real-time similarity search with sub-second response times\n",
        "\n",
        "**Integration with MANVUE:**\n",
        "- Connects to MANVUE MongoDB Atlas database\n",
        "- Generates embeddings for all product images\n",
        "- Provides API-ready search functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: Install Dependencies\n",
        "%pip install pymongo gridfs pillow transformers torch torchvision faiss-cpu datasets scikit-learn\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: Import Required Libraries\n",
        "from pymongo import MongoClient\n",
        "import gridfs\n",
        "import requests, io, datetime, json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import faiss\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: Connect to MongoDB Atlas\n",
        "MONGO_URI = \"mongodb+srv://19276146:19276146@manvue.ilich4r.mongodb.net/?retryWrites=true&w=majority&appName=MANVUE\"\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client[\"MANVUE\"]\n",
        "fs = gridfs.GridFS(db)\n",
        "print(\"‚úÖ Connected to MongoDB Atlas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Setup Instructions\n",
        "\n",
        "Before running the next cells, you need to download and upload the Kaggle Fashion Product Text Images Dataset:\n",
        "\n",
        "### Option 1: Using Kaggle API (Recommended)\n",
        "```python\n",
        "# Install Kaggle API\n",
        "!pip install kaggle\n",
        "\n",
        "# Upload your kaggle.json file (get it from your Kaggle account settings)\n",
        "# Then run:\n",
        "!kaggle datasets download -d nirmalsankalana/fashion-product-text-images-dataset\n",
        "!unzip fashion-product-text-images-dataset.zip\n",
        "```\n",
        "\n",
        "### Option 2: Manual Upload\n",
        "1. Download the dataset from: https://www.kaggle.com/datasets/nirmalsankalana/fashion-product-text-images-dataset\n",
        "2. Upload the extracted folder to your Colab environment\n",
        "3. Update the `DATASET_PATH` variable in the next cell\n",
        "\n",
        "### Expected Dataset Structure:\n",
        "```\n",
        "fashion-product-text-images-dataset/\n",
        "‚îú‚îÄ‚îÄ images/           # Folder containing product images\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ 0.jpg\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ 1.jpg\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îî‚îÄ‚îÄ styles.csv        # CSV file with product metadata\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL: Auto-download Kaggle Dataset\n",
        "# Uncomment and run this cell to automatically download the dataset\n",
        "\n",
        "# !pip install kaggle\n",
        "# !kaggle datasets download -d nirmalsankalana/fashion-product-text-images-dataset\n",
        "# !unzip -q fashion-product-text-images-dataset.zip\n",
        "# !rm fashion-product-text-images-dataset.zip\n",
        "\n",
        "print(\"üìù If you haven't downloaded the dataset yet, please:\")\n",
        "print(\"1. Upload your kaggle.json file to Colab\")\n",
        "print(\"2. Uncomment and run the commands above\")\n",
        "print(\"3. Or manually upload the dataset files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: Load CLIP Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "print(\"‚úÖ CLIP model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: Load Kaggle Fashion Dataset\n",
        "print(\"üì¶ Loading Kaggle fashion product dataset...\")\n",
        "\n",
        "# First, download and extract the dataset from Kaggle\n",
        "# You need to upload the dataset files to your Colab environment\n",
        "# The dataset should contain:\n",
        "# - A folder with product images\n",
        "# - A CSV file with product metadata\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Update these paths based on your uploaded dataset structure\n",
        "DATASET_PATH = \"/content/fashion-product-text-images-dataset\"  # Update this path\n",
        "IMAGES_FOLDER = os.path.join(DATASET_PATH, \"images\")  # Update if different\n",
        "CSV_FILE = os.path.join(DATASET_PATH, \"styles.csv\")  # Update if different\n",
        "\n",
        "# Load the CSV metadata\n",
        "try:\n",
        "    df = pd.read_csv(CSV_FILE)\n",
        "    print(f\"‚úÖ Loaded CSV with {len(df)} products\")\n",
        "    print(f\"üìä Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Display first few rows to understand the structure\n",
        "    print(\"\\nüìã Sample data:\")\n",
        "    print(df.head())\n",
        "    \n",
        "    # Limit to first 500 products for demo (remove this line for full dataset)\n",
        "    df = df.head(500)\n",
        "    print(f\"üì¶ Using {len(df)} products for this demo\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Dataset files not found. Please upload the Kaggle dataset to your Colab environment.\")\n",
        "    print(\"Expected structure:\")\n",
        "    print(\"- /content/fashion-product-text-images-dataset/\")\n",
        "    print(\"  - images/ (folder with product images)\")\n",
        "    print(\"  - styles.csv (CSV file with product metadata)\")\n",
        "    df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 6: Build Embeddings and Save Products in MongoDB\n",
        "print(\"üîÑ Processing images and generating embeddings...\")\n",
        "\n",
        "if df is not None:\n",
        "    image_embeddings = []\n",
        "    metadata = []\n",
        "    \n",
        "    # Common column names in fashion datasets (adjust based on actual CSV structure)\n",
        "    # These are typical column names - update based on your actual CSV\n",
        "    id_col = 'id' if 'id' in df.columns else df.columns[0]\n",
        "    name_col = 'productDisplayName' if 'productDisplayName' in df.columns else 'name'\n",
        "    category_col = 'masterCategory' if 'masterCategory' in df.columns else 'category'\n",
        "    color_col = 'baseColour' if 'baseColour' in df.columns else 'color'\n",
        "    \n",
        "    print(f\"üìä Using columns: ID={id_col}, Name={name_col}, Category={category_col}, Color={color_col}\")\n",
        "    \n",
        "    for i, row in df.iterrows():\n",
        "        try:\n",
        "            # Construct image path - common patterns in fashion datasets\n",
        "            image_id = str(row[id_col])\n",
        "            image_path = os.path.join(IMAGES_FOLDER, f\"{image_id}.jpg\")\n",
        "            \n",
        "            # Try different image extensions if .jpg doesn't exist\n",
        "            if not os.path.exists(image_path):\n",
        "                for ext in ['.png', '.jpeg', '.JPG', '.PNG']:\n",
        "                    alt_path = os.path.join(IMAGES_FOLDER, f\"{image_id}{ext}\")\n",
        "                    if os.path.exists(alt_path):\n",
        "                        image_path = alt_path\n",
        "                        break\n",
        "            \n",
        "            if not os.path.exists(image_path):\n",
        "                print(f\"‚ö†Ô∏è  Image not found for ID {image_id}\")\n",
        "                continue\n",
        "                \n",
        "            # Load and process image\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            \n",
        "            # Get product text description\n",
        "            text = str(row.get(name_col, f\"Product {image_id}\"))\n",
        "            \n",
        "            # Generate CLIP embeddings\n",
        "            inputs = clip_processor(text=[text], images=[img], return_tensors=\"pt\", padding=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = clip_model(**inputs)\n",
        "            \n",
        "            img_emb = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "            emb = img_emb.cpu().numpy()[0]\n",
        "            \n",
        "            # Save image in MongoDB\n",
        "            buf = io.BytesIO()\n",
        "            img.save(buf, format=\"JPEG\")\n",
        "            buf.seek(0)\n",
        "            fs_id = fs.put(buf.read(), filename=f\"product_{image_id}.jpg\", metadata={\n",
        "                \"name\": text,\n",
        "                \"category\": str(row.get(category_col, \"\")),\n",
        "                \"color\": str(row.get(color_col, \"\")),\n",
        "                \"product_id\": image_id\n",
        "            })\n",
        "            \n",
        "            # Store embedding + metadata\n",
        "            image_embeddings.append(emb)\n",
        "            metadata.append({\n",
        "                \"filename\": f\"product_{image_id}.jpg\",\n",
        "                \"name\": text,\n",
        "                \"category\": str(row.get(category_col, \"\")),\n",
        "                \"color\": str(row.get(color_col, \"\")),\n",
        "                \"product_id\": image_id\n",
        "            })\n",
        "            \n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f\"Processed {i+1} items\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing item {i}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if image_embeddings:\n",
        "        image_embeddings = np.array(image_embeddings).astype(\"float32\")\n",
        "        print(f\"‚úÖ Generated embeddings for {len(image_embeddings)} products\")\n",
        "    else:\n",
        "        print(\"‚ùå No embeddings generated. Please check your dataset structure.\")\n",
        "        image_embeddings = []\n",
        "        metadata = []\n",
        "else:\n",
        "    print(\"‚ùå Cannot process images - dataset not loaded\")\n",
        "    image_embeddings = []\n",
        "    metadata = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 7: Save Metadata and Build FAISS Index\n",
        "print(\"üíæ Saving metadata...\")\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "print(\"üîç Building FAISS index...\")\n",
        "dim = image_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(image_embeddings)\n",
        "faiss.write_index(index, \"fashion.index\")\n",
        "\n",
        "print(\"‚úÖ FAISS index built and saved!\")\n",
        "print(f\"üìä Index contains {index.ntotal} vectors with {dim} dimensions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 8: Define Search Function\n",
        "def upload_user_and_find(user_image_path, username=\"guest\", top_k=5):\n",
        "    \"\"\"\n",
        "    Upload a user image and find similar products using CLIP + FAISS\n",
        "    \"\"\"\n",
        "    # Save uploaded image in MongoDB\n",
        "    with open(user_image_path, \"rb\") as f:\n",
        "        img_bytes = f.read()\n",
        "    user_file_id = fs.put(img_bytes, filename=f\"user_{username}_{datetime.datetime.now().timestamp()}.jpg\",\n",
        "                          metadata={\"username\": username, \"upload_time\": datetime.datetime.now()})\n",
        "    \n",
        "    # Get embedding\n",
        "    user_img = Image.open(user_image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(images=[user_img], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    emb = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
        "    emb = emb.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "    # Search FAISS\n",
        "    D, I = index.search(emb, top_k)\n",
        "    results = [metadata[idx] for idx in I[0]]\n",
        "\n",
        "    # Save query + results in MongoDB\n",
        "    query_doc = {\n",
        "        \"username\": username,\n",
        "        \"uploaded_file_id\": str(user_file_id),\n",
        "        \"timestamp\": datetime.datetime.now(),\n",
        "        \"results\": results\n",
        "    }\n",
        "    db[\"queries\"].insert_one(query_doc)\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Search function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 9: Test the Search Function\n",
        "# Upload a sample image to test the search functionality\n",
        "# You can upload any fashion image using the file upload button in Colab\n",
        "\n",
        "# Example usage (uncomment and modify the path when you have an image):\n",
        "# user_image = \"/content/sample_fashion.jpg\"  # Replace with your uploaded image path\n",
        "# results = upload_user_and_find(user_image, username=\"test_user\", top_k=5)\n",
        "# \n",
        "# print(\"üîé Similar Products Found:\")\n",
        "# for i, result in enumerate(results, 1):\n",
        "#     print(f\"{i}. {result['name']} (Category: {result['category']})\")\n",
        "\n",
        "print(\"üìù Ready to test! Upload an image and uncomment the code above to test the search function.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Train Traditional ML Algorithms on CLIP Embeddings\n",
        "\n",
        "Let's train some traditional machine learning algorithms using the CLIP embeddings as features to classify fashion categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import scikit-learn for traditional ML algorithms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"‚úÖ Scikit-learn imported for ML algorithms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset for traditional ML\n",
        "print(\"üîÑ Preparing dataset for machine learning...\")\n",
        "\n",
        "if len(image_embeddings) > 0 and len(metadata) > 0:\n",
        "    # X = embeddings, y = category\n",
        "    X = image_embeddings\n",
        "    y = [m[\"category\"] if m[\"category\"] and m[\"category\"] != \"nan\" else \"unknown\" for m in metadata]\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Split into train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    print(f\"‚úÖ Dataset prepared: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
        "    print(f\"üìä Categories: {np.unique(y)}\")\n",
        "    print(f\"üìä Category distribution: {np.bincount([hash(cat) % len(np.unique(y)) for cat in y])}\")\n",
        "else:\n",
        "    print(\"‚ùå No data available for ML training. Please ensure the dataset was loaded correctly.\")\n",
        "    X_train, X_test, y_train, y_test = None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate multiple ML algorithms\n",
        "if X_train is not None and y_train is not None:\n",
        "    print(\"ü§ñ Training machine learning models...\")\n",
        "    \n",
        "    # 1. Logistic Regression\n",
        "    log_reg = LogisticRegression(max_iter=2000, random_state=42)\n",
        "    log_reg.fit(X_train, y_train)\n",
        "    y_pred_lr = log_reg.predict(X_test)\n",
        "    acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "    print(f\"üìä Logistic Regression Accuracy: {acc_lr:.3f}\")\n",
        "    \n",
        "    # 2. Support Vector Machine\n",
        "    svm_clf = SVC(kernel=\"linear\", random_state=42)\n",
        "    svm_clf.fit(X_train, y_train)\n",
        "    y_pred_svm = svm_clf.predict(X_test)\n",
        "    acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "    print(f\"üìä SVM Accuracy: {acc_svm:.3f}\")\n",
        "    \n",
        "    # 3. k-Nearest Neighbors\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "    acc_knn = accuracy_score(y_test, y_pred_knn)\n",
        "    print(f\"üìä kNN Accuracy: {acc_knn:.3f}\")\n",
        "    \n",
        "    # 4. Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred_rf = rf.predict(X_test)\n",
        "    acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "    print(f\"üìä Random Forest Accuracy: {acc_rf:.3f}\")\n",
        "    \n",
        "    print(\"\\n=== Final Results ===\")\n",
        "    print(f\"Logistic Regression: {acc_lr:.3f}\")\n",
        "    print(f\"SVM: {acc_svm:.3f}\")\n",
        "    print(f\"kNN: {acc_knn:.3f}\")\n",
        "    print(f\"Random Forest: {acc_rf:.3f}\")\n",
        "    \n",
        "    best_model = max([(\"Logistic Regression\", acc_lr), (\"SVM\", acc_svm), \n",
        "                      (\"kNN\", acc_knn), (\"Random Forest\", acc_rf)], key=lambda x: x[1])\n",
        "    print(f\"\\nüèÜ Best performing model: {best_model[0]} with {best_model[1]:.3f} accuracy\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot train ML models - no data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "üéâ **Congratulations!** You've successfully implemented a complete visual search system for MANVUE using the Kaggle Fashion Product Text Images Dataset:\n",
        "\n",
        "### What We Built:\n",
        "1. **CLIP-based Image Embeddings** - Using OpenAI's CLIP model to generate rich image representations\n",
        "2. **FAISS Similarity Search** - Fast, scalable similarity search for finding similar products\n",
        "3. **MongoDB Integration** - Storing images and metadata in GridFS\n",
        "4. **Traditional ML Classification** - Training multiple algorithms on CLIP embeddings\n",
        "5. **Real-time Search API** - Function to upload user images and find similar products\n",
        "\n",
        "### Dataset Used:\n",
        "- **Source**: [Kaggle Fashion Product Text Images Dataset](https://www.kaggle.com/datasets/nirmalsankalana/fashion-product-text-images-dataset)\n",
        "- **Content**: Fashion product images with detailed metadata\n",
        "- **Structure**: Images folder + CSV metadata file\n",
        "- **Scale**: Configurable (demo uses 500 products, can be scaled to full dataset)\n",
        "\n",
        "### Key Features:\n",
        "- ‚úÖ **Flexible dataset loading** - Works with various CSV column structures\n",
        "- ‚úÖ **Robust image processing** - Handles multiple image formats\n",
        "- ‚úÖ **Sub-second search** response times\n",
        "- ‚úÖ **MongoDB storage** for images and metadata\n",
        "- ‚úÖ **Multiple ML algorithms** trained and compared\n",
        "- ‚úÖ **API-ready** search functionality\n",
        "\n",
        "### Next Steps:\n",
        "1. **Download the files**: `fashion.index` and `metadata.json` for your production system\n",
        "2. **Integrate with your API**: Use the search function in your Node.js/Python backend\n",
        "3. **Scale up**: Process the full dataset (remove the `.head(500)` limit)\n",
        "4. **Deploy**: Set up the visual search in your MANVUE e-commerce platform\n",
        "\n",
        "The system is now ready to power visual search in your MANVUE fashion store! üõçÔ∏è\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
