{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fashion Product Dataset MongoDB Loader\n",
        "\n",
        "This notebook loads an already-extracted Fashion Product dataset into MongoDB.\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "```\n",
        "<ROOT_DIR>/\n",
        "  ├─ data.csv           <-- metadata CSV\n",
        "  └─ data/              <-- folder that contains all images (jpg/png/webp)\n",
        "```\n",
        "\n",
        "## What it does\n",
        "- Loads `<ROOT_DIR>/data.csv`\n",
        "- Maps image filenames/paths from the CSV to files inside `<ROOT_DIR>/data/`\n",
        "- Inserts documents into MongoDB\n",
        "\n",
        "## Setup\n",
        "```bash\n",
        "pip install pandas pymongo gridfs\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Iterable, List, Dict\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymongo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MongoClient, ASCENDING\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgridfs\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Optional, Iterable, List, Dict\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient, ASCENDING\n",
        "import gridfs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Edit these variables according to your setup:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== CONFIG (edit these) ==========\n",
        "ROOT_DIR = Path(r\"C:\\Users\\rushi\\Desktop\\technical-test\")   # <-- change to your folder containing data.csv and data/\n",
        "CSV_NAME = \"data.csv\"\n",
        "IMAGES_DIRNAME = \"data\"                      # the folder that has images\n",
        "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".webp\"}\n",
        "\n",
        "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n",
        "DB_NAME = os.getenv(\"MONGO_DB_NAME\", \"fashion_db\")\n",
        "COLLECTION_NAME = os.getenv(\"MONGO_COLLECTION\", \"products\")\n",
        "GRIDFS_BUCKET = os.getenv(\"MONGO_GRIDFS_BUCKET\", \"fs\")\n",
        "\n",
        "STORE_IMAGES_IN_GRIDFS = bool(int(os.getenv(\"STORE_IMAGES_IN_GRIDFS\", \"0\")))  # 0=no, 1=yes\n",
        "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"1000\"))\n",
        "# =========================================\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  ROOT_DIR: {ROOT_DIR}\")\n",
        "print(f\"  CSV_NAME: {CSV_NAME}\")\n",
        "print(f\"  IMAGES_DIRNAME: {IMAGES_DIRNAME}\")\n",
        "print(f\"  MONGO_URI: {MONGO_URI}\")\n",
        "print(f\"  DB_NAME: {DB_NAME}\")\n",
        "print(f\"  COLLECTION_NAME: {COLLECTION_NAME}\")\n",
        "print(f\"  STORE_IMAGES_IN_GRIDFS: {STORE_IMAGES_IN_GRIDFS}\")\n",
        "print(f\"  BATCH_SIZE: {BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def die(msg: str, code: int = 1):\n",
        "    print(f\"[ERROR] {msg}\", file=sys.stderr)\n",
        "    sys.exit(code)\n",
        "\n",
        "\n",
        "def pick_col(df: pd.DataFrame, candidates: Iterable[str]) -> Optional[str]:\n",
        "    \"\"\"Find a column name that matches any of the candidate strings (case-insensitive)\"\"\"\n",
        "    for name in df.columns:\n",
        "        low = name.lower()\n",
        "        if any(key in low for key in candidates):\n",
        "            return name\n",
        "    return None\n",
        "\n",
        "\n",
        "def infer_columns(df: pd.DataFrame):\n",
        "    \"\"\"Automatically infer column names for common fields\"\"\"\n",
        "    col_id = pick_col(df, {\"id\", \"product_id\", \"sku\"})\n",
        "    col_title = pick_col(df, {\"title\", \"name\", \"product_title\"})\n",
        "    col_desc = pick_col(df, {\"description\", \"desc\", \"product_description\", \"text\"})\n",
        "    col_image = pick_col(df, {\"image\", \"image_path\", \"img\", \"filename\", \"file\", \"image_name\", \"image_url\"})\n",
        "    \n",
        "    print(\"[INFO] Inferred columns:\")\n",
        "    print(\"  id      :\", col_id)\n",
        "    print(\"  title   :\", col_title)\n",
        "    print(\"  desc    :\", col_desc)\n",
        "    print(\"  image   :\", col_image)\n",
        "    return col_id, col_title, col_desc, col_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_image_lookup(images_root: Path) -> Dict[str, List[Path]]:\n",
        "    \"\"\"\n",
        "    Build a lookup so we can resolve an image by:\n",
        "      - its basename (e.g., '12345.jpg')\n",
        "      - its stem without extension (e.g., '12345')\n",
        "    We scan only under <ROOT_DIR>/data/.\n",
        "    \"\"\"\n",
        "    if not images_root.exists():\n",
        "        die(f\"Images folder not found: {images_root}\")\n",
        "\n",
        "    lookup: Dict[str, List[Path]] = defaultdict(list)\n",
        "    for p in images_root.rglob(\"*\"):\n",
        "        if p.is_file() and p.suffix.lower() in IMAGE_EXTS:\n",
        "            lookup[p.name].append(p)\n",
        "            lookup[p.stem].append(p)\n",
        "    \n",
        "    print(f\"[INFO] Indexed {sum(len(v) for v in lookup.values())} image entries from {images_root}\")\n",
        "    return lookup\n",
        "\n",
        "\n",
        "def resolve_image_path(value, images_root: Path, image_lookup: Dict[str, List[Path]]) -> Optional[str]:\n",
        "    \"\"\"Resolve image path from various formats (filename, path, etc.)\"\"\"\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    s = str(value).strip()\n",
        "\n",
        "    # Try by basename\n",
        "    base = Path(s).name\n",
        "    if base in image_lookup:\n",
        "        return str(image_lookup[base][0])\n",
        "\n",
        "    # Try by stem (without extension)\n",
        "    stem = Path(s).stem\n",
        "    if stem in image_lookup:\n",
        "        return str(image_lookup[stem][0])\n",
        "\n",
        "    # Try as direct path\n",
        "    candidate = images_root / s\n",
        "    if candidate.exists():\n",
        "        return str(candidate.resolve())\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def content_type_for_suffix(suffix: str) -> str:\n",
        "    \"\"\"Get MIME type for image file extension\"\"\"\n",
        "    s = suffix.lower()\n",
        "    if s == \".png\":\n",
        "        return \"image/png\"\n",
        "    if s == \".webp\":\n",
        "        return \"image/webp\"\n",
        "    return \"image/jpeg\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_records(\n",
        "    df: pd.DataFrame,\n",
        "    images_root: Path,\n",
        "    image_lookup: Dict[str, List[Path]],\n",
        "    col_id: Optional[str],\n",
        "    col_title: Optional[str],\n",
        "    col_desc: Optional[str],\n",
        "    col_image: Optional[str],\n",
        ") -> List[dict]:\n",
        "    \"\"\"Convert DataFrame rows to normalized MongoDB documents\"\"\"\n",
        "    records: List[dict] = []\n",
        "    for _, row in df.iterrows():\n",
        "        rec: dict = {}\n",
        "\n",
        "        # Map columns to standard field names\n",
        "        if col_id and pd.notna(row.get(col_id, None)):\n",
        "            rec[\"product_id\"] = str(row[col_id])\n",
        "        if col_title and pd.notna(row.get(col_title, None)):\n",
        "            rec[\"title\"] = str(row[col_title])\n",
        "        if col_desc and pd.notna(row.get(col_desc, None)):\n",
        "            rec[\"description\"] = str(row[col_desc])\n",
        "\n",
        "        # Resolve image path\n",
        "        img_ref = None\n",
        "        if col_image and col_image in df.columns:\n",
        "            img_ref = resolve_image_path(row[col_image], images_root, image_lookup)\n",
        "        else:\n",
        "            # fallback: try to infer from product_id or title\n",
        "            for key in [row.get(col_id, None), row.get(col_title, None)]:\n",
        "                if pd.notna(key):\n",
        "                    candidate = resolve_image_path(key, images_root, image_lookup)\n",
        "                    if candidate:\n",
        "                        img_ref = candidate\n",
        "                        break\n",
        "\n",
        "        if img_ref:\n",
        "            rec[\"image_path\"] = img_ref\n",
        "            rec[\"image_filename\"] = Path(img_ref).name\n",
        "\n",
        "        # keep original row in case you need extra fields later\n",
        "        rec[\"raw\"] = {k: (None if pd.isna(v) else v) for k, v in row.items()}\n",
        "        records.append(rec)\n",
        "\n",
        "    print(f\"[INFO] Prepared {len(records)} normalized records\")\n",
        "    return records\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MongoDB Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def connect_mongo(uri: str, db_name: str, coll_name: str, gridfs_bucket: str):\n",
        "    \"\"\"Connect to MongoDB and create necessary indexes\"\"\"\n",
        "    client = MongoClient(uri)\n",
        "    db = client[db_name]\n",
        "    coll = db[coll_name]\n",
        "    fs = gridfs.GridFS(db, collection=gridfs_bucket)\n",
        "\n",
        "    # helpful indexes (sparse, not unique by default)\n",
        "    try:\n",
        "        coll.create_index([(\"product_id\", ASCENDING)], sparse=True)\n",
        "        coll.create_index([(\"image_filename\", ASCENDING)], sparse=True)\n",
        "        coll.create_index([(\"title\", ASCENDING)], sparse=True)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Index creation issue:\", e)\n",
        "\n",
        "    print(f\"[INFO] Mongo connected → {db_name}.{coll_name} (docs: {coll.estimated_document_count()})\")\n",
        "    return client, db, coll, fs\n",
        "\n",
        "\n",
        "def put_image_to_gridfs(fs: gridfs.GridFS, path_str: str):\n",
        "    \"\"\"Store image file in GridFS\"\"\"\n",
        "    p = Path(path_str)\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    with open(p, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    return fs.put(\n",
        "        data,\n",
        "        filename=p.name,\n",
        "        contentType=content_type_for_suffix(p.suffix),\n",
        "        metadata={\"source_path\": str(p.resolve())},\n",
        "    )\n",
        "\n",
        "\n",
        "def insert_batched(coll, docs: List[dict], batch_size: int) -> tuple[int, int]:\n",
        "    \"\"\"Insert documents in batches for better performance\"\"\"\n",
        "    inserted, skipped = 0, 0\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        batch = docs[i:i+batch_size]\n",
        "        try:\n",
        "            res = coll.insert_many(batch, ordered=False)\n",
        "            inserted += len(res.inserted_ids)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Batch insert issue ({e}); trying per-document for this batch.\")\n",
        "            for d in batch:\n",
        "                try:\n",
        "                    coll.insert_one(d)\n",
        "                    inserted += 1\n",
        "                except Exception:\n",
        "                    skipped += 1\n",
        "    return inserted, skipped\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution\n",
        "\n",
        "Now let's run the main process to load the data into MongoDB:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Validate paths and load CSV\n",
        "csv_path = ROOT_DIR / CSV_NAME\n",
        "images_root = ROOT_DIR / IMAGES_DIRNAME\n",
        "\n",
        "if not csv_path.exists():\n",
        "    die(f\"CSV not found: {csv_path}\")\n",
        "if not images_root.exists():\n",
        "    die(f\"Images folder not found: {images_root}\")\n",
        "\n",
        "print(\"[INFO] Loading CSV:\", csv_path)\n",
        "df = pd.read_csv(csv_path, low_memory=False)\n",
        "print(\"[INFO] CSV columns:\", list(df.columns))\n",
        "print(f\"[INFO] CSV shape: {df.shape}\")\n",
        "print(f\"[INFO] First few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Infer columns and build image lookup\n",
        "col_id, col_title, col_desc, col_image = infer_columns(df)\n",
        "image_lookup = build_image_lookup(images_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Normalize records\n",
        "records = normalize_records(df, images_root, image_lookup, col_id, col_title, col_desc, col_image)\n",
        "\n",
        "# Show sample of normalized records\n",
        "print(\"\\n[INFO] Sample normalized records:\")\n",
        "for i, record in enumerate(records[:3]):\n",
        "    print(f\"Record {i+1}:\")\n",
        "    print(json.dumps(record, default=str, indent=2))\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Connect to MongoDB\n",
        "client, db, coll, fs = connect_mongo(MONGO_URI, DB_NAME, COLLECTION_NAME, GRIDFS_BUCKET)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Store images in GridFS (if enabled)\n",
        "if STORE_IMAGES_IN_GRIDFS:\n",
        "    print(\"[INFO] Storing images in GridFS …\")\n",
        "    for rec in records:\n",
        "        img_path = rec.get(\"image_path\")\n",
        "        if img_path:\n",
        "            try:\n",
        "                file_id = put_image_to_gridfs(fs, img_path)\n",
        "                if file_id:\n",
        "                    rec[\"image_file_id\"] = file_id\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] GridFS store failed for {img_path}: {e}\")\n",
        "else:\n",
        "    print(\"[INFO] Skipping GridFS storage (STORE_IMAGES_IN_GRIDFS=False)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Insert documents into MongoDB\n",
        "print(\"[INFO] Inserting documents …\")\n",
        "inserted, skipped = insert_batched(coll, records, BATCH_SIZE)\n",
        "print(f\"[DONE] Inserted: {inserted}, Skipped: {skipped}\")\n",
        "print(f\"[INFO] Collection now has {coll.estimated_document_count()} documents.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Show sample document from database\n",
        "sample = coll.find_one({}, projection={\"raw\": False})\n",
        "print(\"[SAMPLE DOC]\")\n",
        "print(json.dumps(sample, default=str, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The notebook has successfully:\n",
        "1. ✅ Loaded the CSV data\n",
        "2. ✅ Built an image lookup index\n",
        "3. ✅ Normalized records for MongoDB\n",
        "4. ✅ Connected to MongoDB\n",
        "5. ✅ Stored images in GridFS (if enabled)\n",
        "6. ✅ Inserted documents in batches\n",
        "7. ✅ Displayed sample results\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "You can now:\n",
        "- Query the MongoDB collection for specific products\n",
        "- Use the GridFS file IDs to retrieve images\n",
        "- Build APIs on top of this data\n",
        "- Perform analytics on the fashion product dataset\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
